% !TeX root = ../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Resumen
%*******************************************************

\chapter{Resumen}

\indent El aumento de la capacidad de cálculo de los ordenadores ha impulsado la necesidad de resolver problemas cada vez más complejos, que dependen de un número creciente de variables. Esto hace necesario diseñar algoritmos capaces de encontrar soluciones para problemas de alta dimensionalidad. Un ejemplo de esto es la optimización de redes neuronales, donde es común tener que optimizar cientos e incluso millones de variables. La utilización de algoritmos clásicos en este contexto no es posible o resulta extremadamente costosa debido al incremento exponencial en la dificultad que trae consigo la alta dimensionalidad. Este problema se conoce como \textit{la maldición de la alta dimensionalidad}. Nos referimos a este fenómeno como la serie de dificultades que aparecen al aumentar el número de dimensiones (o variables) en un espacio, afectando negativamente el rendimiento y la eficacia de los algoritmos de optimización y análisis. A medida que crece la dimensionalidad, las distancias entre puntos pierden significado, el volumen del espacio aumenta exponencialmente, y los datos se dispersan, complicando así su análisis.

En problemas de alta dimensionalidad, muchas técnicas que funcionan eficientemente en espacios de baja dimensión se vuelven ineficaces, ya que los datos ocupan solo una pequeña fracción del espacio total. Esto afecta algoritmos de búsqueda, clasificación y optimización, que requieren más tiempo y recursos para evaluar todas las combinaciones posibles de variables.

En este trabajo proponemos el estudio de una técnica de \textbf{agrupamiento de variables}, que busca separar las variables de un problema en subgrupos independientes, permitiendo optimizar cada subgrupo por separado. Esto reduce significativamente la dimensionalidad efectiva del problema, permitiendo una optimización más eficiente. Al dividir el conjunto de variables en subgrupos independientes, aprovechamos el paralelismo y reducimos la complejidad computacional, ya que cada subgrupo puede optimizarse sin afectar a los demás. Esta técnica es especialmente útil en problemas donde ciertas variables están altamente correlacionadas entre sí, pero tienen poca o ninguna relación con otras variables del conjunto.

Estudiaremos esta técnica aplicándola a algoritmos diseñados para resolver problemas de alta dimensionalidad y a algoritmos que funcionan mejor en baja dimensión. Así, podremos comparar si es más eficiente diseñar algoritmos que mejoren en alta dimensionalidad o dividir el problema en subproblemas menores.

Para ello, introduciremos los conceptos matemáticos necesarios para entender cómo resolver problemas de minimización, así como la teoría detrás de los algoritmos de agrupamiento de variables. Además, analizaremos qué tests estadísticas podemos emplear para obtener una comparación objetiva y efectiva, explicando su selección.

Presentaremos también algoritmos metaheurísticos básicos que más adelante combinaremos para crear nuestra propuesta. Entre ellos, se incluyen \textit{SHADE} y \textit{SHADEils} como algoritmos de optimización, y \textit{ERDG} como algoritmo de agrupamiento de variables.

Adicionalmente, desarrollaremos una biblioteca en el lenguaje de programación \textit{Julia} que implemente los algoritmos utilizados. Esta biblioteca no solo tiene como objetivo evaluar la efectividad de la técnica de agrupamiento de variables, sino también contribuir a la comunidad de \textit{Julia}, proporcionando una herramienta que otros desarrolladores podrán utilizar y mejorar en el futuro.

Para comparar los resultados, utilizaremos el benchmark \textit{CEC2013 LSGO}, diseñado específicamente para problemas de optimización global de alta dimensionalidad. En la obtención y análisis de resultados, utilizaremos la plataforma web \textit{Tacolab}, que permite realizar comparaciones sencillas entre algoritmos. Con los datos obtenidos, determinaremos en qué casos esta técnica puede resultar efectiva y en cuáles no.

\textbf{Palabras clave:} capacidad de cálculo, optimización, redes neuronales, alta dimensionalidad, maldición de la dimensionalidad, agrupamiento de variables, paralelismo, complejidad computacional, algoritmos metaheurísticos, Julia, benchmark, Tacolab.

\endinput

