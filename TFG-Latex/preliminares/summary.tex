% !TeX root = ../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Abstract}

\indent The increase in computational power of computers has enabled the resolution of increasingly complex problems that depend on a growing number of variables. The use of classical algorithms in this context is either infeasible or extremely costly due to the exponential growth in difficulty brought by high dimensionality. This makes it necessary to design algorithms capable of finding solutions to high-dimensional problems. This issue is known as the \textit{curse of high dimensionality}. This phenomenon refers to the exponential growth of the search space as the dimensionality of a problem increases, which severely hinders the ability to find solutions.

In high-dimensional problems, many techniques that work efficiently in low-dimensional spaces become ineffective, as data occupies only a small fraction of the total space. This impacts search, classification, and optimization algorithms, as the growth in the search space leads to an increase in the required computation time, making it impractical to evaluate all possible options.

In this work, we propose the study and implementation of various \textbf{variable grouping} techniques, which aim to separate the variables of a problem into independent subgroups, allowing each subgroup to be optimized separately. This significantly reduces the effective dimensionality of the problem, enabling more efficient optimization. By dividing the set of variables into independent subgroups, we leverage parallelism and reduce computational complexity, as each subgroup can be optimized without affecting the others. This technique is particularly useful in problems where certain variables are highly correlated with each other but have little or no relationship with other variables in the set.

We will study this technique by applying it to algorithms designed to solve high-dimensional problems as well as algorithms that perform better in low dimensions. This will allow us to compare whether it is more efficient to design algorithms that improve performance in high dimensions or to divide the problem into smaller subproblems. To this end, we will introduce the mathematical concepts necessary to understand how to solve minimization problems, as well as the theory behind variable grouping algorithms. Additionally, we will analyze which statistical tests can be used to achieve an objective and effective comparison, explaining the rationale for their selection.

We will also present basic metaheuristic algorithms that we will later combine to develop our proposal. Among these are \textit{SHADE} and \textit{SHADE-ILS} as optimization algorithms, and \textit{ERDG} as a variable grouping algorithm.

Moreover, we will develop a library in the \textit{Julia} programming language to implement the algorithms used. This library aims not only to evaluate the effectiveness of the variable grouping technique but also to contribute to the \textit{Julia} community by providing a tool that other developers can use and improve in the future. To compare results, we will use the \textit{CEC2013 LSGO} benchmark, specifically designed for global optimization problems in high dimensions.

\textbf{Keywords:} computational power, optimization, neural networks, high dimensionality, curse of dimensionality, variable grouping, computational complexity, metaheuristics, Julia.


% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish} 
\endinput
