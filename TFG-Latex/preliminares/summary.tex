% !TeX root = ../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Abstract}

\indent The increase in computing power has driven the need to solve increasingly complex problems that depend on a larger number of variables, making it necessary to design algorithms capable of finding solutions to high-dimensional problems. For example, the optimization of neural networks often involves optimizing hundreds or even millions of variables, which is either impossible or extremely costly using classical algorithms due to the exponential increase in difficulty associated with higher dimensionality. This problem is primarily due to \textit{the curse of dimensionality}. This term refers to the difficulties that arise as the number of dimensions (or variables) in a space increases, negatively impacting the performance and effectiveness of optimization and analysis algorithms. As dimensionality grows, distances between points become less informative, the space's volume increases exponentially, and data becomes increasingly sparse, making analysis more challenging.

In high-dimensional problems, many techniques that work well in low-dimensional spaces become ineffective because the data occupies only a small fraction of the entire space. This impacts algorithms for search, classification, and optimization, which require more time and resources to process and evaluate all possible combinations of variables.

In this work, we propose studying a \textbf{variable grouping} technique that aims to separate the variables of a problem into independent subgroups, allowing each subgroup to be optimized separately. This significantly reduces the effective dimensionality of the problem, enabling more efficient optimization. By dividing the set of variables into independent subgroups, we can leverage parallelism and reduce computational complexity, as each subgroup can be optimized without directly affecting the others. This technique is particularly useful for problems where certain variables are highly correlated with each other but have little or no relation to other variables in the set.

We will study the use of this technique in algorithms specifically designed to solve high-dimensional problems, as well as in algorithms that work better in low dimensions. This will allow us to compare whether it is more efficient to focus on designing algorithms that perform increasingly well in high-dimensional spaces or to break the problem down into smaller subproblems.

To achieve this, we will introduce the necessary mathematical concepts for understanding how to solve minimization problems, as well as the theory behind variable grouping algorithms. We will also analyze which statistical tests we can use to obtain an objective and effective comparison and explain why they were chosen.

We will also present basic metaheuristic algorithms, which we will later combine to create our proposal. These include \textit{SHADE} and \textit{SHADEils} as optimization algorithms and \textit{ERDG} as a variable grouping algorithm.

Additionally, we will develop a library in the \textit{Julia} programming language that implements the algorithms used. This library aims not only to demonstrate whether the variable grouping technique is effective but also to contribute to the \textit{Julia} community by providing a tool that other developers can use and improve upon in the future.

To compare the results, we will use the \textit{CEC2013 LSGO} benchmark, specifically designed for large scale global optimization problems. For results analysis, we will use the \textit{Tacolab} web platform, which allows for straightforward comparisons between algorithms. Based on the data obtained, we will objectively determine in which cases this technique can be effective and in which it cannot.

Keywords: computing power, optimization, neural networks, high dimensionality, curse of dimensionality, variable grouping, parallelism, computational complexity, metaheuristic algorithms, Julia, benchmark, Tacolab.

% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish} 
\endinput
