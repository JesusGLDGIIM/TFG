% !TeX root = ../tfg.tex
% !TeX encoding = utf8

\chapter{Optimización Numérica}

La optimización numérica es una rama fundamental de las matemáticas aplicadas que se dedica al estudio y desarrollo de algoritmos para encontrar los valores óptimos (máximos o mínimos) de funciones, especialmente cuando estas son complejas y no pueden ser resueltas analíticamente \cite{nocedal2006numerical}. Los problemas de optimización aparecen en diversas áreas como la ingeniería, la economía, la física y la inteligencia artificial.

\begin{definicion}
\label{def:optimizacion}
Un \textbf{problema de optimización} consiste en encontrar el vector $\mathbf{x}^* \in \mathbb{R}^n$ que minimiza (o maximiza) una función objetivo $f: \mathbb{R}^n \rightarrow \mathbb{R}$, es decir:
\begin{equation}
\mathbf{x}^* = \arg\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}).
\end{equation}
\end{definicion}

Los problemas de optimización pueden clasificarse en:

\begin{itemize}
    \item \textbf{Optimización sin restricciones}: No existen limitaciones adicionales sobre las variables $\mathbf{x}$.
    \item \textbf{Optimización con restricciones}: Las variables $\mathbf{x}$ deben satisfacer ciertas condiciones, como igualdad o desigualdad.
\end{itemize}

\section{Optimización Sin Restricciones}

En la optimización sin restricciones, el objetivo es encontrar un punto donde la función objetivo alcanza su valor mínimo (o máximo) sin considerar limitaciones adicionales. Matemáticamente, esto implica resolver:
\begin{equation}
\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x}).
\end{equation}

En nuestro caso, consideraremos también problemas de optimización sin restricciones, a los que solo contienen restricciones del tipo:
\begin{equation}
	a \leq x_i \leq b,
\end{equation}
donde \(a\) y \(b\) son números reales, e \(i = 1, 2, \dots, n\), siendo \(n\) la dimensión del espacio. Es decir, las restricciones solo afectan a una única variable. Estas restricciones son útiles en la práctica, ya que no aumentan la complejidad del problema, solo limitan el espacio de búsqueda a una región acotada del espacio, que en la práctica es lo que nos interesa. Además, nos permiten asegurar que, si la función es continua, existe al menos una solución. Por el Teorema de Weierstrass \ref{Weierstrass}, toda función continua definida en un compacto alcanza su máximo y su mínimo. Así que, si definimos una desigualdad del tipo:

\begin{equation}
	a \leq x_i \leq b \quad \forall i,
\end{equation}

tendremos un subconjunto compacto de $\mathbb{R}^n$ y podremos aspirar a encontrar el óptimo global de la función.

\section{Optimización Con Restricciones}

En muchos problemas prácticos, las variables están sujetas a restricciones. Estas pueden ser:

\begin{itemize}
    \item \textbf{Restricciones de igualdad}: $h_i(\mathbf{x}) = 0$, $i = 1, \dots, m$.
    \item \textbf{Restricciones de desigualdad}: $g_j(\mathbf{x}) \leq 0$, $j = 1, \dots, p$.
\end{itemize}

El problema de optimización con restricciones se formula entonces como:
\begin{equation}
\begin{aligned}
& \min_{\mathbf{x} \in \mathbb{R}^n} & & f(\mathbf{x}) \\
& \text{sujeto a} & & h_i(\mathbf{x}) = 0, \quad i = 1, \dots, m, \\
& & & g_j(\mathbf{x}) \leq 0, \quad j = 1, \dots, p.
\end{aligned}
\end{equation}


En nuestro caso, solo estamos interesados en estudiar la optimización numérica sin restricciones, así que nos centraremos en describir las técnicas más utilizadas en este campo sin tener en cuenta las técnicas utilizadas para la optimización con restricciones.

Nuestro objetivo ideal es encontrar un minimizador (para maximizador basta tomar $-f$) global de la función objetivo $f$, es decir, un punto donde la función alcanza su valor mínimo absoluto. Formalmente, definimos:

\begin{definicion}
\label{def:min_global}
Un punto $\mathbf{x}^* \in \mathbb{R}^n$ es un \textbf{minimizador global} de $f$ si
\begin{equation}
f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in \mathbb{R}^n.
\end{equation}
\end{definicion}

Sin embargo, encontrar el minimizador global puede ser difícil debido a la complejidad de $f$. Por lo tanto, a menudo nos conformamos con encontrar un minimizador local.

\begin{definicion}
\label{def:min_local}
Un punto $\mathbf{x}^* \in \mathbb{R}^n$ es un \textbf{minimizador local} de $f$ si existe un entorno $N$ de $\mathbf{x}^*$ tal que
\begin{equation}
f(\mathbf{x}^*) \leq f(\mathbf{x}), \quad \forall \mathbf{x} \in N.
\end{equation}
\end{definicion}

Si la desigualdad es estricta para todos los $\mathbf{x} \neq \mathbf{x}^*$ en $N$, entonces $\mathbf{x}^*$ es un \textbf{minimizador local estricto}.

\section{Condiciones para Óptimos Locales}

Para identificar minimizadores locales, utilizamos condiciones basadas en las derivadas de $f$. Para ello nos será útil el teorema de Taylor \ref{teo:taylor}, cuya demostración se puede encontrar en cualquier libro de cálculo básico.

\subsection{Condiciones de Primer Orden}

A continuación, describimos condiciones necesarias y suficientes para detectar mínimizadores locales de f.

\begin{teorema}[Condiciones Necesarias de Primer Orden]
\label{teo:cond_nec_1}
Si $\mathbf{x}^*$ es un minimizador local de $f$ y $f$ es diferenciable en un entorno abierto de $\mathbf{x}^*$, entonces
\begin{equation}
\nabla f(\mathbf{x}^*) = \mathbf{0}.
\end{equation}
\end{teorema}

\begin{proof}
Supongamos que $\nabla f(\mathbf{x}^*) \neq \mathbf{0}$. Entonces, existe $\mathbf{p} = -\nabla f(\mathbf{x}^*)$ tal que, para $\alpha > 0$ suficientemente pequeño, se tiene $f(\mathbf{x}^* + \alpha \mathbf{p}) < f(\mathbf{x}^*)$, lo cual contradice que $\mathbf{x}^*$ es un minimizador local.
\end{proof}

\subsection{Condiciones de Segundo Orden}

\begin{teorema}[Condiciones Necesarias de Segundo Orden]
\label{teo:cond_nec_2}
Si $\mathbf{x}^*$ es un minimizador local de $f$, $f$ es dos veces diferenciable en un entorno abierto de $\mathbf{x}^*$, y $\nabla f(\mathbf{x}^*) = \mathbf{0}$, entonces
\begin{equation}
\nabla^2 f(\mathbf{x}^*) \text{ es semidefinida positiva}.
\end{equation}
\end{teorema}

\begin{proof}
Si $\nabla^2 f(\mathbf{x}^*)$ no es semidefinida positiva, existe una dirección $\mathbf{p}$ tal que $\mathbf{p}^\top \nabla^2 f(\mathbf{x}^*) \mathbf{p} < 0$. Tomando un paso pequeño en esa dirección, podemos disminuir $f$, lo cual contradice que $\mathbf{x}^*$ es un minimizador local.
\end{proof}

\begin{teorema}[Condiciones Suficientes de Segundo Orden]
\label{teo:cond_suf_2}
Si $\nabla f(\mathbf{x}^*) = \mathbf{0}$ y $\nabla^2 f(\mathbf{x}^*)$ es definida positiva, entonces $\mathbf{x}^*$ es un minimizador local estricto de $f$.
\end{teorema}

\begin{proof}
Dado que el hessiano es definida positiva, para $\mathbf{p} \neq \mathbf{0}$ suficientemente pequeño, el término cuadrático en el desarrollo de Taylor domina, y $f(\mathbf{x}^* + \mathbf{p}) > f(\mathbf{x}^*)$.
\end{proof}

\section{Métodos de Búsqueda en Optimización}

Existen dos estrategias principales para encontrar minimizadores locales en optimización sin restricciones: \textbf{métodos de búsqueda en línea} y \textbf{métodos de región de confianza}.

\subsection{Métodos de Búsqueda en Línea}

En los métodos de búsqueda en línea, cada iteración calcula una dirección de búsqueda $\mathbf{p}_k$ y luego decide cuánto avanzar en esa dirección. La iteración se define como:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k,
\end{equation}
donde $\alpha_k > 0$ es la longitud del paso.

\subsubsection{Selección de la Longitud del Paso}

La elección de $\alpha_k$ es crucial. Idealmente, nos gustaría resolver el problema unidimensional:
\begin{equation}
\min_{\alpha > 0} f(\mathbf{x}_k + \alpha \mathbf{p}_k).
\end{equation}
Sin embargo, resolverlo exactamente puede ser costoso, por lo que a menudo buscamos una aproximación que proporcione una reducción suficiente en $f$ sin incurrir en un alto costo computacional.

\subsubsection{Direcciones de Búsqueda}

La mayoría de los algoritmos de búsqueda en línea requieren que $\mathbf{p}_k$ sea una dirección de descenso, es decir, que satisfaga:
\begin{equation}
\nabla f(\mathbf{x}_k)^\top \mathbf{p}_k < 0.
\end{equation}
Un enfoque común es definir $\mathbf{p}_k$ como:
\begin{equation}
\label{eq:pk_general}
\mathbf{p}_k = -\mathbf{B}_k^{-1} \nabla f(\mathbf{x}_k),
\end{equation}
donde $\mathbf{B}_k$ es una matriz simétrica y no singular.

\paragraph{Descenso de Gradiente}

Si $\mathbf{B}_k = \mathbf{I}$, la identidad, entonces $\mathbf{p}_k = -\nabla f(\mathbf{x}_k)$, que es la dirección de descenso más pronunciado.

\paragraph{Método de Newton}

Si $\mathbf{B}_k = \nabla^2 f(\mathbf{x}_k)$, entonces $\mathbf{p}_k$ es la dirección de Newton:
\begin{equation}
\mathbf{p}_k = -[\nabla^2 f(\mathbf{x}_k)]^{-1} \nabla f(\mathbf{x}_k).
\end{equation}

\subsubsection{Condiciones de Wolfe}

Para asegurar que la longitud del paso $\alpha_k$ proporcione una disminución suficiente en $f$, podemos utilizar las condiciones de Wolfe:

\begin{enumerate}
    \item \textbf{Condición de Decrecimiento Suficiente}:
    \begin{equation}
    \label{CondicionDS}
    f(\mathbf{x}_k + \alpha_k \mathbf{p}_k) \leq f(\mathbf{x}_k) + c_1 \alpha_k \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k,
    \end{equation}
    con $0 < c_1 < 1$.
    \item \textbf{Condición de Curvatura}:
    \begin{equation}
    \label{CondicionCur}
    \nabla f(\mathbf{x}_k + \alpha_k \mathbf{p}_k)^\top \mathbf{p}_k \geq c_2 \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k,
    \end{equation}
    con $c_1 < c_2 < 1$.
\end{enumerate}

Estas condiciones garantizan que $\alpha_k$ no sea ni demasiado pequeño ni demasiado grande, proporcionando un equilibrio entre progreso y estabilidad.

\begin{lema}
\label{lema:wolfe}
Sea $f: \mathbb{R}^n \rightarrow \mathbb{R}$ continuamente diferenciable, y $\mathbf{p}_k$ una dirección de descenso en $\mathbf{x}_k$. Entonces, si $f$ está acotada inferiormente a lo largo de la dirección $\mathbf{p}_k$, existen intervalos de $\alpha_k$ que satisfacen las condiciones de Wolfe.
\end{lema}

\begin{proof}
Dado que $f$ está acotada inferiormente a lo largo de $\mathbf{p}_k$, la función unidimensional $f(\mathbf{x}_k + \alpha \mathbf{p}_k)$ es también acotada inferiormente para $\alpha > 0$. Por otro lado, la línea $f(\mathbf{x}_k) + c_1 \alpha \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k$, que representa la condición de decrecimiento suficiente \ref{CondicionDS}, es no acotada inferiormente cuando $\alpha \to \infty$. Por lo tanto, debe existir al menos un punto $\alpha_0 > 0$ donde ambas gráficas se intersecten, es decir, tal que:
\[
f(\mathbf{x}_k + \alpha_0 \mathbf{p}_k) \leq f(\mathbf{x}_k) + c_1 \alpha_0 \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k.
\]

Ahora, aplicando el teorema del valor medio, existe un $\bar{\alpha} \in (0, \alpha_0)$ tal que:
\[
f(\mathbf{x}_k + \alpha_0 \mathbf{p}_k) = f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k + \bar{\alpha} \mathbf{p}_k)^\top (\alpha_0 \mathbf{p}_k).
\]

Dividiendo por $\alpha_0$ y combinando con la condición de decrecimiento suficiente \ref{CondicionDS}, obtenemos:
\[
\nabla f(\mathbf{x}_k + \bar{\alpha} \mathbf{p}_k)^\top \mathbf{p}_k \geq c_2 \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k,
\]
donde $0 < c_1 < c_2 < 1$. Esto implica que también se satisface la condición de curvatura \ref{CondicionCur}.

Dado que $f$ es continuamente diferenciable, los valores de $\alpha_k$ que satisfacen ambas condiciones \ref{CondicionDS} y \ref{CondicionCur} forman un intervalo no vacío alrededor de $\alpha_0$. Por lo tanto, existe un intervalo de $\alpha_k$ en el cual se satisfacen las condiciones de Wolfe.
\end{proof}


\subsubsection{Algoritmo de Búsqueda de Longitud de Paso}

Presentamos un algoritmo para encontrar $\alpha_k$ que satisface las condiciones de Wolfe.

\begin{algorithm}[H]
\caption{Algoritmo de Búsqueda de Longitud de Paso}
\label{alg:line_search}
\begin{algorithmic}[1]
\STATE Inicializar $\alpha_0 = 0$, $\alpha_{\text{max}} > 0$, elegir $\alpha_1 \in (0, \alpha_{\text{max}})$.
\STATE $i \gets 1$.
\REPEAT
    \STATE Evaluar $f(\mathbf{x}_k + \alpha_i \mathbf{p}_k)$.
    \IF {$f(\mathbf{x}_k + \alpha_i \mathbf{p}_k) > f(\mathbf{x}_k) + c_1 \alpha_i \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k$ \textbf{ó} \\ $[f(\mathbf{x}_k + \alpha_i \mathbf{p}_k) \geq f(\mathbf{x}_k + \alpha_{i-1} \mathbf{p}_k)]$ \textbf{y} $i > 1$}
        \STATE Llamar a \texttt{zoom}($\alpha_{i-1}, \alpha_i$) y \textbf{detener}.
    \ENDIF
    \STATE Evaluar $\nabla f(\mathbf{x}_k + \alpha_i \mathbf{p}_k)$.
    \IF {$|\nabla f(\mathbf{x}_k + \alpha_i \mathbf{p}_k)^\top \mathbf{p}_k| \leq - c_2 \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k$}
        \STATE $\alpha_k \gets \alpha_i$ y \textbf{detener}.
    \ENDIF
    \IF {$\nabla f(\mathbf{x}_k + \alpha_i \mathbf{p}_k)^\top \mathbf{p}_k \geq 0$}
        \STATE Llamar a \texttt{zoom}($\alpha_i, \alpha_{i-1}$) y \textbf{detener}.
    \ENDIF
    \STATE Elegir $\alpha_{i+1} \in (\alpha_i, \alpha_{\text{max}})$.
    \STATE $i \gets i + 1$.
\UNTIL{Condición de terminación}
\end{algorithmic}
\end{algorithm}

\paragraph{Función Zoom}

La función \texttt{zoom} busca un $\alpha_k$ que satisfaga las condiciones de Wolfe dentro del intervalo $[\alpha_{\text{lo}}, \alpha_{\text{hi}}]$.

\begin{algorithm}[H]
\caption{Función \texttt{zoom}($\alpha_{\text{lo}}, \alpha_{\text{hi}}$)}
\label{alg:zoom}
\begin{algorithmic}[1]
\REPEAT
    \STATE Interpolar para encontrar $\alpha_j$ entre $\alpha_{\text{lo}}$ y $\alpha_{\text{hi}}$.
    \STATE Evaluar $f(\mathbf{x}_k + \alpha_j \mathbf{p}_k)$.
    \IF {$f(\mathbf{x}_k + \alpha_j \mathbf{p}_k) > f(\mathbf{x}_k) + c_1 \alpha_j \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k$ \textbf{ó} \\ $f(\mathbf{x}_k + \alpha_j \mathbf{p}_k) \geq f(\mathbf{x}_k + \alpha_{\text{lo}} \mathbf{p}_k)$}
        \STATE $\alpha_{\text{hi}} \gets \alpha_j$.
    \ELSE
        \STATE Evaluar $\nabla f(\mathbf{x}_k + \alpha_j \mathbf{p}_k)$.
        \IF {$|\nabla f(\mathbf{x}_k + \alpha_j \mathbf{p}_k)^\top \mathbf{p}_k| \leq - c_2 \nabla f(\mathbf{x}_k)^\top \mathbf{p}_k$}
            \STATE $\alpha_k \gets \alpha_j$ y \textbf{detener}.
        \ENDIF
        \IF {$(\nabla f(\mathbf{x}_k + \alpha_j \mathbf{p}_k)^\top \mathbf{p}_k)(\alpha_{\text{hi}} - \alpha_{\text{lo}}) \geq 0$}
            \STATE $\alpha_{\text{hi}} \gets \alpha_{\text{lo}}$.
        \ENDIF
        \STATE $\alpha_{\text{lo}} \gets \alpha_j$.
    \ENDIF
\UNTIL{Condición de terminación}
\end{algorithmic}
\end{algorithm}

\section{Métodos de Región de Confianza}

En los métodos de región de confianza, se construye un modelo cuadrático $m_k(\mathbf{p})$ que aproxima $f$ cerca de $\mathbf{x}_k$:
\begin{equation}
m_k(\mathbf{p}) = f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^\top \mathbf{p} + \frac{1}{2} \mathbf{p}^\top \mathbf{B}_k \mathbf{p},
\end{equation}
donde $\mathbf{B}_k$ es una aproximación al hessiano. Se resuelve el subproblema:
\begin{equation}
\min_{\mathbf{p}} m_k(\mathbf{p}), \quad \text{sujeto a} \quad \|\mathbf{p}\| \leq \Delta_k,
\end{equation}
donde $\Delta_k$ es el radio de la región de confianza.

\section{Métodos Quasi-Newton}

Los métodos quasi-Newton buscan aproximar el hessiano sin calcularlo directamente, utilizando únicamente evaluaciones del gradiente.

\subsection{Actualización del Hessiano}

Se basa en la condición de \textbf{secante}:
\begin{equation}
\label{eq:secante}
\mathbf{B}_{k+1} \mathbf{s}_k = \mathbf{y}_k,
\end{equation}
donde:
\begin{align}
\mathbf{s}_k &= \mathbf{x}_{k+1} - \mathbf{x}_k, \\
\mathbf{y}_k &= \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k).
\end{align}

\subsection{El Algoritmo BFGS}

El método BFGS, nombrado por Broyden, Fletcher, Goldfarb y Shanno, es uno de los algoritmos quasi-Newton más populares. Se basa en la construcción de un modelo cuadrático de la función objetivo en el punto actual $\mathbf{x}_k$:
\begin{equation}
m_k(\mathbf{p}) = f_k + \nabla f_k^\top \mathbf{p} + \frac{1}{2} \mathbf{p}^\top \mathbf{B}_k \mathbf{p},
\end{equation}
donde $f_k = f(\mathbf{x}_k)$ y $\nabla f_k = \nabla f(\mathbf{x}_k)$. La matriz $\mathbf{B}_k$ es una aproximación simétrica y definida positiva del hessiano que se actualiza en cada iteración.

La dirección de búsqueda $\mathbf{p}_k$ se obtiene minimizando el modelo cuadrático:
\begin{equation}
\mathbf{p}_k = -\mathbf{B}_k^{-1} \nabla f_k.
\end{equation}

El nuevo punto se calcula como:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k,
\end{equation}
donde $\alpha_k$ es el tamaño de paso determinado mediante una búsqueda en línea que satisface las condiciones de Wolfe.

La actualización de $\mathbf{B}_k$ se realiza utilizando la siguiente fórmula, conocida como la fórmula de BFGS:
\begin{equation}
\mathbf{B}_{k+1} = \mathbf{B}_k - \frac{\mathbf{B}_k \mathbf{s}_k \mathbf{s}_k^\top \mathbf{B}_k}{\mathbf{s}_k^\top \mathbf{B}_k \mathbf{s}_k} + \frac{\mathbf{y}_k \mathbf{y}_k^\top}{\mathbf{y}_k^\top \mathbf{s}_k}.
\end{equation}

Esta fórmula garantiza que $\mathbf{B}_{k+1}$ sea simétrica y definida positiva si $\mathbf{B}_k$ es definida positiva y se satisface la condición de curvatura:
\begin{equation}
\mathbf{s}_k^\top \mathbf{y}_k > 0.
\end{equation}

\begin{algorithm}[H]
\caption{Algoritmo BFGS}
\label{alg:bfgs}
\begin{algorithmic}[1]
\STATE Dado un punto inicial $\mathbf{x}_0$, una tolerancia $\epsilon > 0$, y una matriz inicial $\mathbf{B}_0$ definida positiva.
\STATE $k \gets 0$.
\WHILE {$\|\nabla f_k\| > \epsilon$}
    \STATE Calcular dirección de búsqueda: $\mathbf{p}_k = -\mathbf{B}_k^{-1} \nabla f_k$.
    \STATE Realizar búsqueda en línea para encontrar $\alpha_k$ que satisfaga las condiciones de Wolfe.
    \STATE Actualizar el punto: $\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k$.
    \STATE Calcular $\mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k$ y $\mathbf{y}_k = \nabla f_{k+1} - \nabla f_k$.
    \STATE Actualizar $\mathbf{B}_{k+1}$ usando la fórmula de BFGS.
    \STATE $k \gets k + 1$.
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\section{Optimización en alta dimensionalidad}

Para problemas donde el número de variables es elevado, técnicas como BFGS se vuelven inmanejables debido a que requieren manipular y almacenar matrices densas de gran tamaño. Esto puede causar problemas de rendimiento o de memoria, especialmente en sistemas con recursos limitados. Para abordar estas limitaciones, se desarrollaron versiones adaptadas como el algoritmo L-BFGS, diseñado específicamente para optimización en alta dimensionalidad.

\subsection{El Algoritmo L-BFGS}

El algoritmo L-BFGS (\textit{Limited-memory Broyden-Fletcher-Goldfarb-Shanno}) es una técnica eficiente para problemas de optimización de gran escala, donde almacenar y manipular la matriz hessiana completa es impráctico. Este método utiliza un número limitado de pares de vectores $\{\mathbf{s}_i, \mathbf{y}_i\}$ provenientes de las iteraciones más recientes. Estos pares capturan la información de curvatura necesaria para actualizar la dirección de búsqueda, mientras que la información más antigua se descarta para ahorrar memoria. Esto permite reducir significativamente los requerimientos de almacenamiento y cómputo, manteniendo una tasa de convergencia aceptable.

A continuación, se presenta el algoritmo L-BFGS:

\begin{algorithm}[H]
\caption{Algoritmo L-BFGS}
\label{alg:l-bfgs-formal}
\begin{algorithmic}[1]
\STATE Elegir un punto inicial $\mathbf{x}_0$, un entero $m > 0$ y un criterio de convergencia.
\STATE Inicializar $k \gets 0$.
\REPEAT
    \STATE Elegir la matriz inicial $\mathbf{H}_0^k$ (por ejemplo, usando la ecuación \eqref{eq:scaling-h0}).
    \STATE Calcular la dirección de búsqueda $\mathbf{p}_k$ usando la \textit{recursión de dos bucles} (ver Algoritmo \ref{alg:two-loop}).
    \STATE Actualizar el punto: $\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k$, donde $\alpha_k$ satisface las condiciones de Wolfe.
    \STATE Calcular los nuevos pares de vectores:
    \[
    \mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k, \quad \mathbf{y}_k = \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k).
    \]
    \IF{$k \geq m$}
        \STATE Eliminar el par más antiguo $\{\mathbf{s}_{k-m}, \mathbf{y}_{k-m}\}$ de la memoria.
    \ENDIF
    \STATE Almacenar el nuevo par $\{\mathbf{s}_k, \mathbf{y}_k\}$.
    \STATE Incrementar el contador: $k \gets k + 1$.
\UNTIL{Se cumple el criterio de convergencia.}
\end{algorithmic}
\end{algorithm}

\subsubsection{Recursión de dos bucles}

El cálculo de la dirección de búsqueda $\mathbf{p}_k$ se realiza mediante la \textit{recursión de dos bucles}, que calcula el producto $\mathbf{H}_k \nabla f_k$ sin construir explícitamente $\mathbf{H}_k$. Este procedimiento es esencial para mantener la eficiencia computacional del método. A continuación, se describe el proceso:

\begin{algorithm}[H]
\caption{Recursión de dos bucles para L-BFGS}
\label{alg:two-loop}
\begin{algorithmic}[1]
\STATE Dado $\nabla f_k$, los pares $\{\mathbf{s}_i, \mathbf{y}_i\}$, y la matriz inicial $\mathbf{H}_0^k$.
\STATE Inicializar $\mathbf{q} \gets \nabla f_k$.
\FOR{$i = k-1$ hasta $\max(k-m, 0)$}
    \STATE $\alpha_i \gets \frac{\mathbf{s}_i^\top \mathbf{q}}{\mathbf{y}_i^\top \mathbf{s}_i}$.
    \STATE $\mathbf{q} \gets \mathbf{q} - \alpha_i \mathbf{y}_i$.
\ENDFOR
\STATE $\mathbf{r} \gets \mathbf{H}_0^k \mathbf{q}$.
\FOR{$i = \max(k-m, 0)$ hasta $k-1$}
    \STATE $\beta \gets \frac{\mathbf{y}_i^\top \mathbf{r}}{\mathbf{y}_i^\top \mathbf{s}_i}$.
    \STATE $\mathbf{r} \gets \mathbf{r} + \mathbf{s}_i (\alpha_i - \beta)$.
\ENDFOR
\STATE Devolver $\mathbf{p}_k \gets -\mathbf{r}$.
\end{algorithmic}
\end{algorithm}

\subsubsection{Elección de la matriz inicial $\mathbf{H}_0^k$}

La matriz inicial $\mathbf{H}_0^k$ se utiliza para escalar la dirección de búsqueda y mejorar la convergencia del algoritmo. Una elección común es:
\begin{equation}
\label{eq:scaling-h0}
\mathbf{H}_0^k = \gamma_k \mathbf{I},
\end{equation}
donde
\begin{equation}
\gamma_k = \frac{\mathbf{s}_{k-1}^\top \mathbf{y}_{k-1}}{\mathbf{y}_{k-1}^\top \mathbf{y}_{k-1}}.
\end{equation}
Esta estrategia permite capturar información de curvatura reciente, asegurando que la dirección de búsqueda esté bien escalada.

\subsection{Ventajas del L-BFGS}

\begin{itemize}
	\item \textbf{Memoria limitada:} El algoritmo almacena solo los últimos $m$ pares $\{\mathbf{s}_i, \mathbf{y}_i\}$, lo que reduce significativamente los requisitos de almacenamiento comparado con BFGS estándar.
	\item \textbf{Eficiencia computacional:} La recursión de dos bucles tiene un costo de $4mn$ operaciones, donde $n$ es el número de variables y $m$ el número de pares almacenados.
	\item \textbf{Aplicaciones prácticas:} Es ampliamente utilizado en problemas de optimización de alta dimensionalidad, como el ajuste de parámetros en modelos de aprendizaje automático y simulaciones numéricas.
\end{itemize}

\endinput
%--------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%--------------------------------------------------------------------